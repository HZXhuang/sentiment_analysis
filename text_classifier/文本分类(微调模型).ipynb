{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b7a0ddb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入相关的库\n",
    "import functools\n",
    "import os\n",
    "\n",
    "import paddle\n",
    "import paddle.nn.functional as F\n",
    "from paddle.io import BatchSampler, DataLoader\n",
    "from utils import preprocess_function, read_local_dataset\n",
    "\n",
    "from paddlenlp.data import DataCollatorWithPadding\n",
    "from paddlenlp.datasets import load_dataset, MapDataset\n",
    "from paddlenlp.transformers import AutoModelForSequenceClassification, AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c946e2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对单条语句进行主题分类\n",
    "def predict_classify(sentence):\n",
    "    \"\"\"\n",
    "    Predicts the data labels.\n",
    "    \"\"\"\n",
    "    paddle.set_device(\"gpu\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"./checkpoint/\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"./checkpoint/\")\n",
    "\n",
    "    label_list = []\n",
    "    label_path = os.path.join(\"data\", \"label.txt\")\n",
    "    with open(label_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            label_list.append(line.strip())\n",
    "\n",
    "    data_ds = MapDataset([{\"sentence\": sentence}])\n",
    "\n",
    "    trans_func = functools.partial(\n",
    "        preprocess_function,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=128,\n",
    "        label_nums=len(label_list),\n",
    "        is_test=True,\n",
    "    )\n",
    "\n",
    "    data_ds = data_ds.map(trans_func)\n",
    "\n",
    "    # batchify dataset\n",
    "    collate_fn = DataCollatorWithPadding(tokenizer)\n",
    "    data_batch_sampler = BatchSampler(data_ds, batch_size=16, shuffle=False)\n",
    "\n",
    "    data_data_loader = DataLoader(dataset=data_ds, batch_sampler=data_batch_sampler, collate_fn=collate_fn)\n",
    "\n",
    "    results = []\n",
    "    model.eval()\n",
    "    for batch in data_data_loader:\n",
    "        logits = model(**batch)\n",
    "        probs = F.sigmoid(logits).numpy()\n",
    "        for prob in probs:\n",
    "            labels = []\n",
    "            for i, p in enumerate(prob):\n",
    "                if p > 0.5:\n",
    "                    labels.append(i)\n",
    "            results.append(labels)\n",
    "    predict_labels = []\n",
    "    for d, result in zip(data_ds.data, results):\n",
    "        label = [label_list[r] for r in result]\n",
    "        predict_labels.append(\",\".join(label))\n",
    "    return predict_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ff25ef47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(shape=[1, 7], dtype=float32, place=Place(gpu:0), stop_gradient=False,\n",
      "       [[-3.67354250, -4.37633419,  3.20282364, -4.19195175, -4.40284300,\n",
      "         -3.83011103, -4.48348856]])\n",
      "[[0.02475787 0.01241528 0.9609404  0.01489164 0.01209442 0.02124601\n",
      "  0.01116782]]\n"
     ]
    }
   ],
   "source": [
    "batch = data_data_loader.__iter__().__next__()\n",
    "logits = model(**batch)\n",
    "print(logits)\n",
    "probs = F.sigmoid(logits).numpy()\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "338a8e46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': '达叔: 看到'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ds = load_dataset(\n",
    "    read_local_dataset, path=os.path.join(\"data\", \"data.txt\"), is_test=True, lazy=False\n",
    ")\n",
    "data_ds.__getitem__(1)\n",
    "# data_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6f042c8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': Tensor(shape=[1, 7], dtype=int64, place=Place(gpu:0), stop_gradient=True,\n",
       "        [[1   , 302 , 1985, 12049, 335 , 45  , 2   ]]),\n",
       " 'token_type_ids': Tensor(shape=[1, 7], dtype=int64, place=Place(gpu:0), stop_gradient=True,\n",
       "        [[0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "55d5f3ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': '达叔: 看到'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat = MapDataset([{\"sentence\": \"达叔: 看到\"}])\n",
    "dat.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e26156c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2023-08-29 12:58:48,793] [    INFO]\u001b[0m - We are using <class 'paddlenlp.transformers.ernie.modeling.ErnieForSequenceClassification'> to load './checkpoint/'.\u001b[0m\n",
      "\u001b[32m[2023-08-29 12:58:48,794] [    INFO]\u001b[0m - Loading configuration file ./checkpoint/config.json\u001b[0m\n",
      "\u001b[32m[2023-08-29 12:58:48,796] [    INFO]\u001b[0m - Loading weights file ./checkpoint/model_state.pdparams\u001b[0m\n",
      "\u001b[32m[2023-08-29 12:58:49,250] [    INFO]\u001b[0m - Loaded weights file from disk, setting weights to model.\u001b[0m\n",
      "\u001b[32m[2023-08-29 12:58:50,184] [    INFO]\u001b[0m - All model checkpoint weights were used when initializing ErnieForSequenceClassification.\n",
      "\u001b[0m\n",
      "\u001b[32m[2023-08-29 12:58:50,185] [    INFO]\u001b[0m - All the weights of ErnieForSequenceClassification were initialized from the model checkpoint at ./checkpoint/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ErnieForSequenceClassification for predictions without further training.\u001b[0m\n",
      "\u001b[32m[2023-08-29 12:58:50,212] [    INFO]\u001b[0m - We are using <class 'paddlenlp.transformers.ernie.tokenizer.ErnieTokenizer'> to load './checkpoint/'.\u001b[0m\n",
      "D:\\env\\anaconda3\\envs\\pytorch\\lib\\site-packages\\paddlenlp\\transformers\\tokenizer_utils_base.py:2296: FutureWarning: The `max_seq_len` argument is deprecated and will be removed in a future version, please use `max_length` instead.\n",
      "  FutureWarning,\n",
      "D:\\env\\anaconda3\\envs\\pytorch\\lib\\site-packages\\paddlenlp\\transformers\\tokenizer_utils_base.py:1866: UserWarning: Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "  \"Truncation was not explicitly activated but `max_length` is provided a specific value, \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['人物塑造,价值观念,影视特效,故事情节,演员演技']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 预测单个句子\n",
    "predict_classify(\"中国科幻电影: 严重滞后于世界\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9764fe6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
